{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory Data Analysis And Data Preprocessing :\n",
    "Pre-processing refers to the transformations applied to the data before feeding it to the algorithm. Data Preprocessing Includes Data Collection , Cleaning ,Conversion of features , Imputing missing data ,features standardizations , feature scaling and identify new potential features.\n",
    "\n",
    "#### Source of Data Set :\n",
    "   For this article, I have used a Data From (Need to be added ) data set from You can download the final training and testing data set from here:           \n",
    "                \n",
    "Lets get started by importing important packages and the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named deprecation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-da7c90620512>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda2\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[1;31m# avoid flakes unused variable error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda2\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChangedBehaviorWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_ChangedBehaviorWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named deprecation"
     ]
    }
   ],
   "source": [
    "%pylab inline \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats, integrate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "# special matplotlib argument for improved plots\n",
    "from matplotlib import rcParams\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#import xgboost as xgb\n",
    "#from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "\n",
    "matplotlib.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring The Variables :\n",
    "The first step in exploratory analysis is reading in the data and then exploring the variables. It is important to get a sense of how many variables and cases there are, the data types of the variables and the range of values they take on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data uploded into the data frame  \n",
    "loan_data_df  = pd.read_csv('LoansTrainingSet.csv')\n",
    "\n",
    "#convert the loan status into number;\n",
    "loan_data_df['flag'] = loan_data_df['Loan Status'].replace(['Charged Off' ,'Fully Paid'], \n",
    "                     [1, 0]) ;\n",
    "\n",
    "# Rename All Columns :\n",
    "loan_data =loan_data_df.rename(columns = {'Loan ID':'loan_id' ,\\\n",
    "                                          'Customer ID' :'customer_id' ,\\\n",
    "                                          'Loan Status' :'loan_status',\\\n",
    "                                          'Current Loan Amount' : 'current_loan_amount' ,\\\n",
    "                                          'Term' :'term',\\\n",
    "                                          'Credit Score':'credit_score' ,\\\n",
    "                                          'Years in current job' : 'years_in_current_job',\\\n",
    "                                          'Home Ownership' :'home_ownership' ,\\\n",
    "                                          'Annual Income' :'annual_income' ,\\\n",
    "                                          'Purpose' :'purpose',\\\n",
    "                                          'Monthly Debt' :'monthly_debt',\\\n",
    "                                          'Years of Credit History' :'years_of_credit_history' ,\\\n",
    "                                          'Months since last delinquent' :'months_since_last_delinquent',\n",
    "                                          'Number of Open Accounts' : 'number_of_open_accounts' ,\n",
    "                                          'Number of Credit Problems': 'number_of_credit_problems',\n",
    "                                          'Current Credit Balance' : 'current_credit_balance' ,\n",
    "                                          'Maximum Open Credit' :'maximum_open_credit',\n",
    "                                          'Bankruptcies':'bankruptcies',\n",
    "                                          'Tax Liens' :'tax_liens',\n",
    "                                          'flag' :'defaulter_or_not'\n",
    "\n",
    "                                           \n",
    "                                           })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Removing non numeric characters from the features columns\n",
    "# This should be uncomment\n",
    "loan_data['purpose'] =   map(lambda x: x.lower(), loan_data['purpose'])\n",
    "#loan_data['monthly_debt'] = [x[1:] for x in loan_data['monthly_debt']]\n",
    "#loan_data['monthly_debt'] = loan_data[\"monthly_debt\"].str.replace(\",\", \"\").astype(float)\n",
    "#loan_data = loan_data[loan_data['maximum_open_credit'] != '#VALUE!']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \" Dimensions of Data \"\n",
    "print loan_data.shape           # Check dimensions\n",
    "print \"Data types : \"\n",
    "print loan_data.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intially From the Source Data There are 20 Features and 256984 observations .Several of the column variables are encoded as numeric data types (ints and floats) but a few of them are encoded as \"object\". Let's check the head of the data to get a better sense of what the variables look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(loan_data.head(2))  # Check the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears we have a mixture of numeric columns and columns with text data. In data analysis, variables that split records into a fixed number of unique categories, such as Sex, are known as categorical variables. Pandas will attempt to interpret categorical variables as such when you load data, but you can specifically convert a variable to categorical if necessary, as we'll see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting a sense of the data's structure, it is a good idea to look at a statistical summary of the variables with df.describe():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print( loan_data.describe() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noticeable that non-numeric columns are dropped from the statistical summary provided by loan_data.describe().There are some non numerical values for this summary may not reflect the proper \n",
    "We can get a summary of the categorical variables by passing only those columns to describe():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical = loan_data.dtypes[loan_data.dtypes == \"object\"].index\n",
    "print(categorical)\n",
    "\n",
    "loan_data[categorical].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although describe() gives a concise overview of each variable, it does not necessarily give us enough information to determine what each variable means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loan ID: A unique Identifier for the loan information.\n",
    "\n",
    "# Customer ID: A unique identifier for the customer. Customers may have more than one loan.\n",
    "\n",
    "# Loan Status: A categorical variable indicating if the loan was paid back or defaulted.\n",
    "\n",
    "# Current Loan Amount: This is the loan amount that was either completely paid off, or the amount that was defaulted.\n",
    "\n",
    "# Term: A categorical variable indicating if it is a short term or long term loan.\n",
    "\n",
    "# Credit Score: A value between 0 and 800 indicating the riskiness of the borrowers credit history.\n",
    "\n",
    "# Years in current job: A categorical variable indicating how many years the customer has been in their current job.\n",
    "\n",
    "# Home Ownership: Categorical variable indicating home ownership. Values are \"Rent\", \"Home Mortgage\", and \"Own\". If the value is OWN, then the customer is a home owner with no mortgage\n",
    "\n",
    "# Annual Income: The customer's annual income\n",
    "\n",
    "# Purpose: A description of the purpose of the loan.\n",
    "\n",
    "# Monthly Debt: The customer's monthly payment for their existing loans\n",
    "\n",
    "# Years of Credit History: The years since the first entry in the customer’s credit history •\n",
    "# Months since last delinquent: Months since the last loan delinquent payment\n",
    "\n",
    "# Number of Open Accounts: The total number of open credit cards\n",
    "\n",
    "#  Number of Credit Problems: The number of credit problems in the customer records.\n",
    "\n",
    "# Current Credit Balance: The current total debt for the customer\n",
    "\n",
    "#  Maximum Open Credit: The maximum credit limit for all credit sources.\n",
    "\n",
    "# Bankruptcies: The number of bankruptcies\n",
    "\n",
    "# Tax Liens: The number of tax liens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After looking at the data for the first time, you should ask yourself a few questions:\n",
    "\n",
    "   1. Do I need all of the variables?\n",
    "   2. Should I transform any variables?\n",
    "   3. Are there NA values, outliers or other strange values?\n",
    "   4. Should I create new variables?\n",
    "\n",
    "For the rest of this lesson we will address each of these questions in the context of this data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove All variables that are necessary :\n",
    "Getting rid of unnecessary variables is a good first step when dealing with any data set, since dropping variables reduces complexity and can make computation on the data faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del loan_data[\"customer_id\"]\n",
    "del loan_data[\"loan_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there NA Values, Outliers or Other Strange Values?\n",
    "Data sets are often littered with missing data, extreme data points called outliers and other strange values. Missing values, outliers and strange values can negatively affect statistical tests and models and may even cause certain functions to fail.\n",
    "In Python, you can detect missing values with the pd.isnull() function:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Detecting Missing Values :\n",
    "  let us first find out how many values are missing . Missing values shows the data discripency. Let us see whether we can find some detail insight about the data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loan_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above chart we can find the  there are 61676 observations where credit score and annual income are not available . let us find how many of them are defaulter or not .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_cr_score =   loan_data[pd.isnull(loan_data[\"credit_score\"])] \n",
    "#missing['loan_status'] = missing['loan_status'].astype(str)\n",
    "missing_cr_score= pd.Categorical(missing_cr_score['loan_status'] )\n",
    "\n",
    "m  =  pd.DataFrame(missing_cr_score.describe())\n",
    "m\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.freqs.plot(kind =\"bar\" , color =\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately  31% loan has been charged off where credit score is not available . Let us explore annual_income and find how \n",
    "missing values impact there ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_annual_income =   loan_data[pd.isnull(loan_data[\"annual_income\"])] \n",
    "#missing['loan_status'] = missing['loan_status'].astype(str)\n",
    "missing_annual_income = pd.Categorical(missing_annual_income['loan_status'] )\n",
    "\n",
    "m  =  pd.DataFrame(missing_annual_income.describe())\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar impact like credit score .so later we will try to explore relationship between annual_income and credit_score variables ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing =   loan_data[pd.isnull(loan_data[\"months_since_last_delinquent\"])] \n",
    "#missing['loan_status'] = missing['loan_status'].astype(str)\n",
    "missing = pd.Categorical(missing['loan_status'] )\n",
    "\n",
    "m  =  pd.DataFrame(missing.describe())\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.freqs.plot(kind =\"bar\" , color =\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Variables in the Data Set :\n",
    "There are four categorical Variables in the data set . Term of loans  , Purpose of loans , Home ownership and years in the current job . Among them though here years in current job is categorical variables but we will convert it to continuous as this will have significant impact on credit risk modeling .\n",
    "\n",
    "There are two types of loan . short term and long term . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "def calculate_amount(r ) :\n",
    "    return  r *1.0/ len(loan_data) \n",
    "\n",
    "\n",
    "\n",
    "ct = pd.crosstab(index=loan_data[\"term\"], \n",
    "                          columns=loan_data[\"loan_status\"] ).apply(lambda x : calculate_amount(x ) , axis = 1)\n",
    "\n",
    "\n",
    "ct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct.plot(kind=\"bar\", \n",
    "                 \n",
    "                 stacked=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term loan has more vulnerable approximately term loan given to 25% people among them 48% are defaulter which is 12 % of given data set . It seems Short term loan are more safe as it has 20% defaulter which is 26% of the given short term loan . It is provided that 32%  among provided data set are defaulter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = pd.crosstab(index=loan_data[\"purpose\"], \n",
    "                          columns=loan_data[\"loan_status\"] ).apply(lambda x : calculate_amount(x ) , axis = 1)\n",
    "\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct.plot(kind=\"bar\", \n",
    "                 \n",
    "                 stacked=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apporximately 80% loans are used for debt consolidation and 25% are defaulter . other loans are very nominal as home improvments amd misc. loans are in second and 3rd positions respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = pd.crosstab(index=loan_data[\"home_ownership\"], \n",
    "                          columns=loan_data[\"loan_status\"] ).apply(lambda x : calculate_amount(x ) , axis = 1)\n",
    "\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct.plot(kind=\"bar\", \n",
    "                 \n",
    "                 stacked=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though most loans has been provided to them who has house mortgage . but most defaulter are those who live in house\n",
    "with Rent . 15% are defaulter which  is more than 34 % of that loan which has been provided to people with rent house"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We could leave 'years_in_current_job' as categorical data, but it shouldn't be treated as such or as ordinal data since the intervals are easy to determine. We can convert it into numerical data with a simple filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = pd.crosstab(index=loan_data[\"years_in_current_job\"], \n",
    "                          columns=loan_data[\"loan_status\"]  ).apply(lambda x : calculate_amount(x *100) , axis = 1)\n",
    "\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct.plot(kind=\"bar\", \n",
    "                 \n",
    "                 stacked=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Above graph it is found that most of the loans given to the employee who have more than 10+ years experience . Since It is a numerica variables we will convert it to numeric values ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loan_data.replace('n/a', np.nan,inplace=True)\n",
    "loan_data.years_in_current_job.fillna(value=0,inplace=True)\n",
    "loan_data['years_in_current_job'].replace(to_replace='[^0-9]+', value='', inplace=True, regex=True)\n",
    "loan_data['years_in_current_job'] = loan_data['years_in_current_job'].astype(int)\n",
    "loan_data.years_in_current_job.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding categorical features :\n",
    "We have 3 Categorical features .Purpose , Term , home_ownership ,years_in_current_job . We will convert them into distinct features using one hot encoding method and later we will drop the parent column.Before drop this let us understand what is the impact of this features into the data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(df, colname ) :\n",
    "    df[colname] =  df[colname].str.lower()\n",
    "    df[colname] =  df[colname].str.replace(\" \" ,\"_\")\n",
    "    df = pd.concat([df, pd.get_dummies(df[colname]).rename(columns=lambda x:   str(x))], axis=1) \n",
    "    df.drop(colname,axis =1 , inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "loan_data = one_hot_encoding(loan_data, 'purpose' ) ;\n",
    "loan_data =  one_hot_encoding(loan_data, 'term' ) ;\n",
    "loan_data = one_hot_encoding(loan_data, 'home_ownership' ) ;\n",
    "\n",
    "del loan_data[\"loan_status\"]\n",
    "\n",
    "\n",
    "\n",
    "print loan_data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring Continuous Data :\n",
    "Discrete data are, again, data with a finite or countably infinite number of possible outcomes. Continuous data, on the other hand, are data which come from an interval of possible outcomes.In our Data set examples of continuous variables include :\n",
    "   - Current loan amount           \n",
    "   - Credit score                  \n",
    "   - Years in current job          \n",
    "   - Annual income                 \n",
    "   - Monthly debt                  \n",
    "   - Years of credit history       \n",
    "   - Months since last delinquent  \n",
    "   - Number of open accounts       \n",
    "   - Number of credit problems     \n",
    "   - Current credit balance        \n",
    "   - Maximum open credit      \n",
    "   \n",
    "In each of these examples, the resulting measurement comes from an interval of possible outcomes. and the measurement tool is often the restricting factor with continuous data. We'll summarize the data graphically using histograms, stem-and-leaf plots, and box plots.Here, we'll investigate how to summarize continuous data numerically using order statistics and various functions of order statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Imputing missing values :\n",
    "Data sets are often littered with missing data, extreme data points called outliers and other strange values. Missing values, outliers and strange values can negatively affect statistical tests and models and may even cause certain functions to fail.\n",
    "\n",
    "Detecting missing values is the easy part: it is far more difficult to decide how to handle them. In cases where we have a lot of data and only a few missing values, it might make sense to simply delete records with missing values present. On the other hand, if you have more than a handful of missing values, removing records with missing values could cause you to get rid of a lot of data. Missing values in categorical data are not particularly troubling because we can simply treat NA as an additional category. Missing values in numeric variables are more troublesome, since you can't just treat a missing value as number. \n",
    "\n",
    "Here are a few ways we could deal with them:\n",
    "1. Replace the null values with 0s\n",
    "2. Replace the null values with some central value like the mean or median\n",
    "3. Impute values (estimate values using statistical/predictive modeling methods.).\n",
    "\n",
    "This example shows that imputing the missing values can give better results than discarding the samples containing any missing value. Sometimes dropping rows or using marker values is more effective. Here we will tackle missing values with mean values of that column. So there is no missing Values .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "loan_data['annual_income'].fillna((loan_data['annual_income'].mean()), inplace=True)\n",
    "loan_data['months_since_last_delinquent'].fillna((loan_data['months_since_last_delinquent'].mean()), inplace=True)\n",
    "loan_data['credit_score'].fillna((loan_data['credit_score'].mean()), inplace=True)\n",
    "loan_data['bankruptcies'].fillna((loan_data['bankruptcies'].median()), inplace=True)\n",
    "loan_data['tax_liens'].fillna((loan_data['tax_liens'].median()), inplace=True)\n",
    "loan_data['maximum_open_credit'].fillna((0), inplace=True)\n",
    "\n",
    "loan_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will explore all continuous variables and explore all outliers ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "defaulter = loan_data[['current_loan_amount' ,'credit_score','years_in_current_job','annual_income','monthly_debt',\\\n",
    "                       'years_of_credit_history', \\\n",
    "                      'months_since_last_delinquent', 'number_of_open_accounts' ,'number_of_credit_problems' ,\\\n",
    "                       'current_credit_balance' ,  'maximum_open_credit']].loc[loan_data['defaulter_or_not'] == 1]\n",
    "\n",
    "print defaulter.info()\n",
    "\n",
    "non_defaulter = loan_data[['current_loan_amount' ,'credit_score','years_in_current_job','annual_income','monthly_debt'\\\n",
    "                           ,'years_of_credit_history', \\\n",
    "                      'months_since_last_delinquent', 'number_of_open_accounts' ,'number_of_credit_problems' ,\\\n",
    "                       'current_credit_balance' ,  'maximum_open_credit']].loc[loan_data['defaulter_or_not'] == 0]\n",
    "\n",
    "print non_defaulter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eda_continuous_var(colname) :\n",
    "    f, axarr = plt.subplots(3, 3)\n",
    "    f.set_size_inches(15, 9)\n",
    "    ax1 = axarr[0, 0]\n",
    "    ax1.set_title('Axis [0,0]')\n",
    "    defaulter.hist(column=colname,        # Column to plot\n",
    "                          # Plot size\n",
    "                      color=\"red\" ,\n",
    "                      bins=50 ,ax =ax1) \n",
    "\n",
    "    non_defaulter.hist(column=colname,        # Column to plot\n",
    "                          # Plot size\n",
    "                      color=\"blue\" ,\n",
    "                      bins=100,ax = axarr[0, 1]) \n",
    "\n",
    "    loan_data.hist(column = colname,        # Column to plot\n",
    "                          # Plot size\n",
    "                      color=\"green\" ,\n",
    "                      bins=100,ax = axarr[0, 2]) \n",
    "    \n",
    "    defaulter[colname].plot(kind =\"box\" , ax = axarr[1,0 ] , color =\"red\")\n",
    "    non_defaulter[colname].plot(kind =\"box\" , ax = axarr[1,1] , color =\"blue\")\n",
    "    loan_data[colname].plot(kind =\"box\" , ax = axarr[1,2 ] , color =\"green\")\n",
    "    \n",
    "    plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)\n",
    "    plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "eda_continuous_var('current_loan_amount') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Between  Variables :\n",
    "Here we will learn how to statistically test for a relationship between two categorical variables. Categorical variables are related when the category that a subject is classified as for one variable \"influences\" the category that a subject is classified as in another variable. In other words, the category one is classified as for one variable \"depends\" on the category one is classified as in another variable. The chi-square (χ2χ2) test of independence will be used to test for this relationship.\n",
    "\n",
    "#### Chi-Square Test of Independence :\n",
    "The chi-square (χ2) test of independence is used to test for a statistically significant relationship between two categorical variables. This is an inferential test that uses data from a sample to make conclusions about the relationship between categorical variables in the population.\n",
    "\n",
    "The chi-square distribution is a special type of right skewed distribution.  Like the t distribution, the chi-square distribution varies depending on the degrees of freedom. \n",
    "\n",
    "An observed relationship will be called statistically significant when the p-value for a chi-square test is less than αα (typically α=.05α=.05). In this case, if we reject the null hypothesis, then we generalize that there is a relationship in the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Apply Machine Learning Model to Data Set :\n",
    "To predict the class of the loan we will use supervised model for classification .our goal is to predict class level which is a choice of predefined list of possiblities . Our goal is to make accuratepredictions for new, never-before-seen data. Supervised learning oftenrequires human effort to build the training set, but afterwardautomates and often speeds up an otherwise laborious or infeasible task.For now we will go through the algorithm themselves by first .\n",
    "\n",
    "#### k-Nearest Neighbours :\n",
    "The k-NN algorithm is arguably the simplest machinelearning algorithm. Building the model consists only of storing the training dataset. To make a prediction for a new data point, the  algorithm finds the closest data points in the training dataset—its “nearest neighbors.”\n",
    "##### How do we choose the factor K?\n",
    "we can see that the boundary becomes smoother with increasing value of K. With K increasing to infinity it finally becomes all blue or all red depending on the total majority.  The training error rate and the validation error rate are two parameters we need to access on different K-value. Following is the curve for the training error rate with varying value of K :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = list(loan_data.columns)\n",
    "features.remove('defaulter_or_not');\n",
    "\n",
    "target =  'defaulter_or_not'\n",
    "\n",
    "\n",
    "X =  loan_data[list(features)].values\n",
    "y = loan_data[target].values\n",
    "\n",
    "X_samples = X[1:50000]\n",
    "y_samples = y[1:50000]\n",
    "\n",
    "\n",
    "\n",
    "train_X, test_X, train_Y, test_Y =  train_test_split(X_samples, y_samples, test_size= .5, random_state=66)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "training_accuracy = []\n",
    "test_accuracy = []# try n_neighbors from 1 to 10\n",
    "neighbors_settings = range(1, 11)\n",
    "for n_neighbors in neighbors_settings:    # build the model   \n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)    \n",
    "    clf.fit(train_X, train_Y)    # record training set accuracy\n",
    "    training_accuracy.append(clf.score(train_X, train_Y))    # record generalization accuracy   \n",
    "    test_accuracy.append(clf.score(test_X, test_Y))\n",
    "\n",
    "plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From the above diagram we can find that for K=1 we get training accuracy 100 % but test accuracy less than 75%. if k=8 to 10 we get less difference between test and train data . for our modelling we will use k=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STRENGTHS, WEAKNESSES, AND PARAMETERS of kNN :\n",
    "In principle, there are two important parameters to the KNeighbors classifier: \n",
    "1. the number of neighbors \n",
    "2. how we measure distance between data points. In practice, using a small number of neighbors like three or five often works well, but you should certainly adjust this parameter. \n",
    "\n",
    "There are several advantages of Knn :\n",
    "The model is very easy to understand, and often gives reasonable performance without a lot of adjustments .Using this algorithm is a good baseline method to try before considering mIore advanced techniques.\n",
    "On the other hand though it is fast but for larger training set prediction may be slow .It does not perform well when data set have many features . In these later method we will try to reduce or overcome the limitation of Knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LINEAR MODELS FOR CLASSIFICATION :\n",
    "The two most common linear classification algorithms are logistic regression, implemented in linear_model.LogisticRegression, and linear support vector machines (linear SVMs), implemented in svm.LinearSVC (SVC stands for support vector classifier).\n",
    "\n",
    "The two models come up with similar decision boundaries. Note that both misclassify two of the points. By default, both models apply an L2 regularization, in the same way that Ridge does for regression.\n",
    "\n",
    "For LogisticRegression and LinearSVC the trade-off parameter that determines the strength of the regularization is called C, and higher values of C correspond to less regularization. In other words, when you use a high value for the parameter C, LogisticRegression and LinearSVC try to fit the training set as best as possible, while with low values of the parameter C, the models put more emphasis on finding a coefficient vector (w) that is close to zero.\n",
    "\n",
    "There is another interesting aspect of how the parameter C acts. Using low values of C will cause the algorithms to try to adjust to the “majority” of data points, while using a higher value of C stresses the importance that each individual data point be classified correctly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "training_accuracy = []\n",
    "test_accuracy = []# try n_neighbors from 1 to 10\n",
    "c_settings = range(1,100 ,10)\n",
    "for c in c_settings:    # build the model   \n",
    "    clf = LogisticRegression(C=c)    \n",
    "    clf.fit(train_X, train_Y)    # record training set accuracy\n",
    "    training_accuracy.append(clf.score(train_X, train_Y))    # record generalization accuracy   \n",
    "    test_accuracy.append(clf.score(test_X, test_Y))\n",
    "\n",
    "plt.plot(c_settings, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(c_settings, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"C = regularization Parameter\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this Logistic regression we are plotting and finding now much variation . though training accuracy is less then test accuracy ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifiers :\n",
    "Naive Bayes classifiers are a family of classifiers that are quite similar to the linear models discussed in the previous section. However, they tend to be even faster in training. The price paid for this efficiency is that naive Bayes models often provide generalization performance that is slightly worse than that of linear classifiers like LogisticRegression and LinearSVC.\n",
    "\n",
    "The reason that naive Bayes models are so efficient is that they learn parameters by looking at each feature individually and collect simple per-class statistics from each feature. \n",
    "\n",
    "##### STRENGTHS, WEAKNESSES, AND PARAMETERS :\n",
    "MultinomialNB and BernoulliNB have a single parameter, alpha, which controls model complexity. The way alpha works is that the algorithm adds to the data alpha many virtual data points that have positive values for all the features. This results in a “smoothing” of the statistics. A large alpha means more smoothing, resulting in less complex models. \n",
    "\n",
    "The naive Bayes models share many of the strengths and weaknesses of the linear models. They are very fast to train and to predict, and the training procedure is easy to understand. The models work very well with high-dimensional sparse data and are relatively robust to the parameters. Naive Bayes models are great baseline models and are often used on very large datasets, where training even a linear model might take too long.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees :\n",
    "   Decision tree is a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator in input variables.\n",
    "   \n",
    "How does a tree decide where to split : \n",
    "   The decision of making strategic splits heavily affects a tree’s accuracy. Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.  \n",
    "   \n",
    "   Gini Index :\n",
    "   if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure.\n",
    "   \n",
    "   Chi-Square :\n",
    "   It is an algorithm to find out the statistical significance between the differences between sub-nodes and parent node. We measure it by sum of squares of standardized differences between observed and expected frequencies of target variable.\n",
    "   \n",
    "   Information Gain:\n",
    "   Look at the image below and think which node can be described easily. I am sure, your answer is C because it requires less information as all values are similar.\n",
    "   \n",
    "   Reduction in Variance :\n",
    "   Till now, we have discussed the algorithms for categorical target variable. Reduction in variance is an algorithm used for continuous target variables (regression problems). This algorithm uses the standard formula of variance to choose the best split. The split with lower variance is selected as the criteria to split the population:\n",
    "   \n",
    "   \\begin{equation*}\n",
    "   varience = \\frac{\\displaystyle\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}\n",
    "   \\end{equation*}\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "training_accuracy = []\n",
    "test_accuracy = []# try n_neighbors from 1 to 10\n",
    "max_depth_settings = range(1,100 ,2)\n",
    "for c in max_depth_settings:    # build the model   \n",
    "    clf = DecisionTreeClassifier(random_state=0 , max_depth=c ,max_leaf_nodes = 20 ,criterion =  \"entropy\" )   \n",
    "    clf.fit(train_X, train_Y)    # record training set accuracy\n",
    "    training_accuracy.append(clf.score(train_X, train_Y))    # record generalization accuracy   \n",
    "    test_accuracy.append(clf.score(test_X, test_Y))\n",
    "\n",
    "plt.plot(max_depth_settings, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(max_depth_settings, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"C = Maximum Depth\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STRENGTHS, WEAKNESSES, AND PARAMETERS :\n",
    "As discussed earlier, the parameters that control model complexity in decision trees are the pre-pruning parameters that stop the building of the tree before it is fully developed. Usually, picking one of the pre-pruning strategies—setting either max_depth, max_leaf_nodes, or min_samples_leaf—is sufficient to prevent overfitting.\n",
    "\n",
    "Advantages includes :\n",
    "the resulting model can easily be visualized and understood by nonexperts (at least for smaller trees), and the algorithms are completely invariant to scaling of the data. As each feature is processed separately, and the possible splits of the data don’t depend on scaling, no preprocessing like normalization or standardization of features is needed for decision tree algorithms. In particular, decision trees work well when you have features that are on completely different scales, or a mix of binary and continuous features.\n",
    "\n",
    "The main downside of decision trees is that even with the use of pre-pruning, they tend to overfit and provide poor generalization performance. Therefore, in most applications, the ensemble methods we discuss next are usually used in place of a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model :\n",
    "Bagging is a technique used to reduce the variance of our predictions by combining the result of multiple classifiers modeled on different sub-samples of the same data set. The following figure will make it clearer .\n",
    "The steps followed in bagging are:\n",
    "\n",
    "1. Create Multiple DataSets:\n",
    "Sampling is done with replacement on the original data and new datasets are formed.The new data sets can have a fraction of the columns as well as rows, which are generally hyper-parameters in a bagging model.\n",
    "Taking row and column fractions less than 1 helps in making robust models, less prone to overfitting\n",
    "2. Build Multiple Classifiers:\n",
    "Classifiers are built on each data set.\n",
    "Generally the same classifier is modeled on each data set and predictions are made.\n",
    "3. Combine Classifiers:\n",
    "The predictions of all the classifiers are combined using a mean, median or mode value depending on the problem at hand.\n",
    "The combined values are generally more robust than a single model.\n",
    "\n",
    "#### STRENGTHS, WEAKNESSES, AND PARAMETERS :\n",
    "\n",
    "This algorithm can solve both type of problems i.e. classification and regression and does a decent estimation at both fronts.\n",
    "One of benefits of Random forest which excites me most is, the power of handle large data set with higher dimensionality. It can handle thousands of input variables and identify most significant variables so it is considered as one of the dimensionality reduction methods. Further, the model outputs Importance of variable, which can be a very handy feature (on some random data set).\n",
    "\n",
    "#### Extreme Gradient Boosting (xgboost) :\n",
    "\n",
    "Extreme Gradient Boosting (xgboost) is similar to gradient boosting framework but more efficient. It has both linear model solver and tree learning algorithms. So, what makes it fast is its capacity to do parallel computation on a single machine.\n",
    "\n",
    "This makes xgboost at least 10 times faster than existing gradient boosting implementations. It supports various objective functions, including regression, classification and ranking.\n",
    "\n",
    "Since it is very high in predictive power but relatively slow with implementation, “xgboost” becomes an ideal fit for many competitions. It also has additional features for doing cross validation and finding\n",
    "Gradient Boosted Regression Trees :\n",
    "\n",
    "\n",
    "Strengths, weaknesses, and parameters\n",
    "Gradient boosted decision trees are among the most powerful and widely used models for supervised learning. Their main drawback is that they require careful tuning of the parameters and may take a long time to train. Similarly to other tree-based models, the algorithm works well without scaling and on a mixture of binary and continuous features. As with other tree-based models, it also often does not work well on high-dimensional sparse data.\n",
    "\n",
    "The main parameters of gradient boosted tree models are the number of trees, n_estimators, and the learning_rate, which controls the degree to which each tree is allowed to correct the mistakes of the previous trees. These two parameters are highly interconnected, as a lower learning_rate means that more trees are needed to build a model of similar complexity. In contrast to random forests, where a higher n_estimators value is always better, increasing n_estimators in gradient boosting leads to a more complex model, which may lead to overfitting. A common practice is to fit n_estimators depending on the time and memory budget, and then search over different learning_rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer Perceptron (MLP)  :\n",
    "\n",
    "Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function by training on a dataset, where m is the number of dimensions for input and o is the number of dimensions for output. Given a set of features X = {x_1, x_2, ..., x_m} and a target y, it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers. \n",
    "\n",
    "The advantages of Multi-layer Perceptron are:\n",
    "   1. Capability to learn non-linear models.\n",
    "   2. Capability to learn models in real-time (on-line learning) using partial_fit.\n",
    "\n",
    "The disadvantages of Multi-layer Perceptron (MLP) include:\n",
    "1. MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.\n",
    "2. MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n",
    "3. MLP is sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation : \n",
    "In classification problems, we use two types of algorithms (dependent on the kind of output it creates):\n",
    "\n",
    "Class output : Algorithms like SVM and KNN create a class output. For instance, in a binary classification problem, the outputs will be either 0 or 1. However, today we have algorithms which can convert these class outputs to probability. But these algorithms are not well accepted by the statistics community.\n",
    "\n",
    "Probability output : Algorithms like Logistic Regression, Random Forest, Gradient Boosting, Adaboost etc. give probability outputs. Converting probability outputs to class output is just a matter of creating a threshold probability.\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "A confusion matrix is an N X N matrix, where N is the number of classes being predicted. For the problem in hand, we have N=2, and hence we get a 2 X 2 matrix. Here are a few definitions, you need to remember for a confusion matrix :\n",
    "\n",
    "Accuracy : the proportion of the total number of predictions that were correct.\n",
    "Positive Predictive Value or Precision : the proportion of positive cases that were correctly identified.\n",
    "Negative Predictive Value : the proportion of negative cases that were correctly identified.\n",
    "Sensitivity or Recall : the proportion of actual positive cases which are correctly identified.\n",
    "Specificity : the proportion of actual negative cases which are correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Benchmark classifiers\n",
    "def benchmark(clf, name ):\n",
    "\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    if name == \"XGboost Classifier\" :\n",
    "        clf.fit(train_X, train_Y ,eval_metric='auc' )\n",
    "    else :\n",
    "        clf.fit(train_X, train_Y)\n",
    "            \n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "    print()\n",
    "    t0 = time()\n",
    "    y_pred = clf.predict(test_X)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "    \n",
    "    score = metrics.accuracy_score(test_Y, y_pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "    roc = roc_auc_score(test_Y, y_pred)\n",
    "    print (\"AUC Score (Roc): %0.3f\" % roc )\n",
    "    recall = recall_score(test_Y, y_pred)\n",
    "\n",
    "    print (\"recall score :%0.3f \" % recall )\n",
    "    f1 = f1_score(test_Y, y_pred) \n",
    "    print (\"F1 score :%0.3f\"  %  f1 )\n",
    "    precision = precision_score(test_Y, y_pred)\n",
    "    print (  \"Precision score : %0.3f \" % precision )\n",
    "    \n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr,score , train_time ,test_time , roc ,  recall , f1, precision  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "for clf, name in ( (LogisticRegression() ,\"Logistic Regression \"),\n",
    "        (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "        (DecisionTreeClassifier(min_samples_split =5 ,max_depth= 10 ),\n",
    "         p   ),\n",
    "        \n",
    "        (RandomForestClassifier(n_estimators=15), \"Random forest\"),\n",
    "                  ( XGBClassifier(\n",
    "                     learning_rate =0.1,\n",
    "                     n_estimators=50,\n",
    "                     max_depth=5,\n",
    "                     min_child_weight=1,\n",
    "                     gamma=0,\n",
    "                     subsample=0.8,\n",
    "                     colsample_bytree=0.8,\n",
    "                     objective= 'binary:logistic',\n",
    "                     nthread=4,\n",
    "                     scale_pos_weight=1,\n",
    "                     seed=27) ,\"XGboost Classifier\" )\n",
    "                 \n",
    "                 \n",
    "                 \n",
    "                 \n",
    "                 ):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf ,name ))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "indices = np.arange(len(results))\n",
    "#print (results)\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(8 )]\n",
    "\n",
    "\n",
    "clf_names, score, training_time, test_time,  roc ,  recall , f1, precision  = results\n",
    "training_time = np.array(training_time) / np.max(training_time)\n",
    "test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, score, .1, label=\"score\", color='r')\n",
    "plt.barh(indices + .1, training_time, .1, label=\"training time\", color='g')\n",
    "plt.barh(indices + .2, test_time, .1, label=\"test time\", color='b')\n",
    "plt.barh(indices + .3, roc , .1, label=\"ROC\", color='m')\n",
    "plt.barh(indices + .4, recall , .1, label=\"recall\", color='y')\n",
    "plt.barh(indices + .5, f1 , .1, label=\"f1\", color='k')\n",
    "plt.barh(indices + .6, f1 , .1, label=\"precision\", color='c')\n",
    "\n",
    "\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "plt.subplots_adjust(left=.\n",
    "                    25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i, c)\n",
    "\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area Under the ROC curve (AUC – ROC) :\n",
    "The ROC curve is the plot between sensitivity and (1- specificity). (1- specificity) is also known as false positive rate and sensitivity is also known as True Positive rate. Following is the ROC curve for the case in hand.\n",
    "\n",
    "he sensitivity at this threshold is 99.6% and the (1-specificity) is ~60%. This coordinate becomes on point in our ROC curve. To bring this curve down to a single number, we find the area under this curve (AUC).\n",
    "\n",
    "Note that the area of entire square is 1*1 = 1. Hence AUC itself is the ratio under the curve and the total area. For the case in hand, we get AUC ROC as 96.4%. Following are a few thumb rules:\n",
    "\n",
    ".90-1 = excellent (A)\n",
    ".80-.90 = good (B)\n",
    ".70-.80 = fair (C)\n",
    ".60-.70 = poor (D)\n",
    ".50-.60 = fail (F)\n",
    "We see that we fall under the excellent band for the current model. But this might simply be over-fitting. In such cases it becomes very important to to in-time and out-of-time validations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
