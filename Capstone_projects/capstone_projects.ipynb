{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Science Capstone - Milestone Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation :\n",
    "we need to add some line here\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Understanding the Problem :\n",
    "Problem Definition and summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Acquisition and Cleaning \n",
    " * Remove Non Ascii Characters : Consider to remove all non ascii characters\n",
    " * Case Consideration: \n",
    " No case will Be considered .All words will be converted into lower case.         \n",
    " * Remove Stopwords :\n",
    " similarly, unlike classification and clustering applications, all words will be included in the model as they represent more than just the primary carriers of the message.\n",
    " * Wordform : stemming will not be used as N-Grams are typically based on wordforms (unique, inflected\n",
    "forms of words). Whereas table and tables are the same lemma, they will be treated as separate words in\n",
    "this model .\n",
    " * Removing punctuations : need to remove All punctuation remards except   \" ' \" .\n",
    " * Removing Numbers: there is no intuition based on the research that numbers will have a great impact on a predication\n",
    "model and they will be removed\n",
    " * Removing Whitespace: this was not discussed directly by Jurafsky and Martin. The intuition is that whitespace has\n",
    "little to do with context and excess whitespace will be removed\n",
    " * Sparse Words: all words will be retained.\n",
    " * Typo correction : For one of the complete datasets the function which_misspelled from the ‘qdap’ package was used to determine the most frequent typos/error. Apparently, the most common errors is the omission of the apostroph (e.g. “Im” instead of “I’m” or “theyve” instead of “they’ve”) and the use of ‘internet slang’ abbreviations (e.g. ‘idk’ for “I don’t know”). Also, a lot of links to internet pages are present. Therefore, to allow for some cleaning of the data, these common terms will be replaced (non-exhaustive list shown below).\n",
    " * Profanity Filtering : the external link to a textfile with ‘bad words’ was used for profanity filtering (removal of offensive words).\n",
    " \n",
    "     \n",
    " \n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['know', \"'s\", \"re'ason\", 'proceed', 'https', 'asap']\n",
      "['know', \"'s\", \"re'ason\", 'proceed', 'https', 'asap']\n"
     ]
    }
   ],
   "source": [
    "# remove non ascii characters \n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from HTMLParser import HTMLParser\n",
    "def remove_non_ascii(text):\n",
    "    return unidecode(unicode(text, encoding = \"utf-8\"))\n",
    "\n",
    "\n",
    "# codes from http://stackoverflow.com/a/12824140/735926\n",
    "RE_EMOJI = re.compile(\n",
    "        \"[\"\n",
    "            r\"\\U00002600-\\U000026FF\"  # Misc Symbols\n",
    "            r\"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "            r\"\\U0001F300-\\U0001F5FF\"  # Misc Symbols & Pictographs\n",
    "            r\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "            r\"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "        \"]\"\n",
    ", re.UNICODE)\n",
    "\n",
    "\n",
    "# URLs + http:... at the end of truncated tweets\n",
    "RE_HTTP = re.compile(r\"\\b(?:https?://\\S+|https?:\\.\\.\\.$)\")\n",
    "\n",
    "RE_SPACES = re.compile(r\"\\s+\")\n",
    "RE_NUMBERS = re.compile(\"\\d+\")\n",
    "\n",
    "RE_HTML_ENTITY = re.compile(r\"&[a-z]+;\")\n",
    "\n",
    "RE_RT = re.compile(r\"^RT @\\S+:?\")\n",
    "RE_END_HASHTAGS = re.compile(r\"(?:#\\S+\\s*)+$\")\n",
    "RE_HASH = re.compile(r\"#\")\n",
    "RE_MENTION = re.compile(r\"@\\S+\")\n",
    "\n",
    "# some tweets contain lots of hashs, e.g.: ####### and this confuse the\n",
    "# RE_END_HASHTAGS replacement that takes a lot of time. We add this one before\n",
    "# to remove these repetitions\n",
    "RE_HASHS = re.compile(r\"##+\")\n",
    "\n",
    "\n",
    "SLANGS = [(re.compile(r\"\\b%s\\b\" % re.escape(s)),r) for s,r in (\n",
    "    (\"cuz\", \"because\"),\n",
    "    (\"u r\", \"you are\"),\n",
    "    (\"thx\", \"thanks\"),\n",
    "\n",
    "    # we use legit words instead of Twitter-specific language here\n",
    "    (\"DM\", \"private message\"),\n",
    "    (\"RT\", \"write\"),\n",
    "    (\"tweet\", \"message\"),\n",
    ")]\n",
    "\n",
    "DEL_SLANGS = [re.compile(s, re.IGNORECASE) for s in (\n",
    "    \"asap\",\n",
    "    \"lol\",\n",
    "    \"rofl\",\n",
    ")]\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return filtered_words\n",
    "\n",
    "def remove_punctuation(words) :\n",
    "    clean_words = []\n",
    "    for word in words :\n",
    "        word =   re.sub('[^A-Za-z\\']+', '', word).strip()\n",
    "        if  word != '': \n",
    "            clean_words.append(word)\n",
    "    return clean_words\n",
    "\n",
    "words = ['kno04w', \"'s\", '``', 're\\'ason', \".......\", '.' ,'?', 'pro~~ceed',\":(\",\"https:\",\"asap\"]\n",
    "print remove_punctuation(words)\n",
    "\n",
    "\n",
    "\n",
    "NAMES_BLACKLIST = set([\n",
    "    \"(\", \")\", \"[\", \"]\", \"+\", \"|\", \"%\", \"...\", \"~\", \"_\", \":[\",\n",
    "    \"The\", \"Of\", \"In\", \"\", \"n\", \"t\", \"\\n\", \"m\", \"x\", \"N\",\n",
    "    \"ll\", \"re\", \"ve\", \"pm\",\n",
    "])\n",
    "\n",
    "htmlparser = HTMLParser()\n",
    "\n",
    "\n",
    "def compile_OR_pattern(patterns, *args):\n",
    "    s = \"(?:%s)\" % \"|\".join([re.escape(p) for p in patterns])\n",
    "    return re.compile(s, *args)\n",
    "\n",
    "\n",
    "def contains_emoji(text):\n",
    "    \"\"\"\n",
    "    Test if some text contains emojis.\n",
    "    \"\"\"\n",
    "    return RE_EMOJI.search(text) is not None\n",
    "\n",
    "\n",
    "def unslangize(text):\n",
    "    \"\"\"\n",
    "    Try to remove as many slang words as possible, replacing with their correct\n",
    "    form.\n",
    "    \"\"\"\n",
    "    for s in DEL_SLANGS:\n",
    "        text = s.sub(\"\", text)\n",
    "\n",
    "    for reg, rep in SLANGS:\n",
    "        text = reg.sub(rep, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Return a normalized version of a status, meant to be tokenized by NLTK for\n",
    "    entity extraction.\n",
    "    Note that on an Homebrewed Python on OS X the function might print warnings\n",
    "    like: ::\n",
    "        RuntimeWarning: Surrogate character u'\\\\udf05' will be ignored\n",
    "    You can safely ignore them.\n",
    "    \"\"\"\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "print normalize_text(words) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "blogpath = \"./final/en_US/sample.txt\"\n",
    "\n",
    "newspath = \"./final/en_US/en_US.news.txt\"\n",
    "twitterpath = \"./final/en_US/en_US.twitter.txt\"\n",
    "\n",
    "\n",
    "blog_num_lines = 0\n",
    "blog_num_words = 0\n",
    "blog_num_chars = 0\n",
    "blog_max_line_length = 0\n",
    "\n",
    "columns = ['File Name','Size ','Documents','Vocabulary (V)','Max line Length' ,'Word Types (T)','TTR (T/V)','Diversity']\n",
    "\n",
    "words = []\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "with open(twitterpath) as fin:\n",
    "    for line in fin :\n",
    "        blog_num_lines += 1\n",
    "        line = remove_non_ascii(line) # remove non ascii characters\n",
    "        line = line.lower() #convert into lower case letter\n",
    "        \n",
    "        tokens = nltk.word_tokenize(line)\n",
    "        #print tokens\n",
    "        tokens = remove_stopwords(tokens) #remove stop words\n",
    "        #print \"after removing stop words:---------\"\n",
    "        #print tokens\n",
    "        tokens = remove_punctuation(tokens) # remove punctuation and whitespaces and clean numbers\n",
    "        #print \"after removing punctuations:---------\"\n",
    "        #print tokens\n",
    "        words =  words + tokens\n",
    "\n",
    "        \n",
    "        #unique_words.add (line.split())\n",
    "        blog_num_words += len(words)\n",
    "        blog_num_chars = len(tokens)\n",
    "        if blog_max_line_length < blog_num_chars :\n",
    "            blog_max_line_length = blog_num_chars\n",
    "\n",
    "#print ( \"number of lines in blog file : \" , blog_num_lines )\n",
    "#print ( \"number of words in blog file : \" , blog_num_words )\n",
    "\n",
    "\n",
    "unique_words =  set(words)\n",
    "print len(unique_words)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "print \"Total Unique Words \" , len(unique_words) ,\"Total words \" , len(words)\n",
    "df_ = pd.DataFrame([['Twitter', 0,blog_num_lines ,blog_num_words ,  blog_max_line_length,0,0,0 ] ],columns=columns)\n",
    "df_ \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Assume all data are clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_csv.reader' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a1a3f2f8f341>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Split full comments into sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Append SENTENCE_START and SENTENCE_END\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"%s %s %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msentence_start_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_end_token\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_csv.reader' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import csv\n",
    " \n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print \"Reading CSV file...\"\n",
    "with open('./final/input.txt', 'rb') as f:\n",
    "    line = f.readLine()\n",
    "    \n",
    "    reader.next()\n",
    "     \n",
    "    # Split full comments into sentences\n",
    "    sentence = nltk.sent_tokenize(reader.decode('utf-8').lower())\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentence]\n",
    "print \"Parsed %d sentences.\" % (len(sentences))\n",
    "print sentences[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
